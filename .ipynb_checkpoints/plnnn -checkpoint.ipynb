{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nXS4RUQQugV"
   },
   "source": [
    "# Tutorial Part-of-Speech tagging  Con Deep Learning\n",
    "\n",
    "### En este tutorial, veremos cómo puede usar un modelo simple en Keras, para entrenar y evaluar una red neuronal artificial  BLSTM para problemas de clasificación de múltiples clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIheRrq2Quga"
   },
   "source": [
    "## PARTE 1  -  Pre-Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lw10qukzQuge"
   },
   "outputs": [],
   "source": [
    "# Asegurar reproducibilidad\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "CUSTOM_SEED = 42\n",
    "np.random.seed(CUSTOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxOcxDUmJcvL"
   },
   "source": [
    "### Descargamos el Corpus Ancora - Cess_esp del nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NILevBJxQugr",
    "outputId": "05db9f45-57c6-4420-f290-fd9be6a8978b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to /home/daniel/nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/daniel/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('cess_esp')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqdUvUCEJjgc"
   },
   "source": [
    "### Extraemos las oraciones tageadas del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ss3RHo4LQugx"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import cess_esp\n",
    "\n",
    "tagged_sentences1 = brown.tagged_sents()#[:6030]\n",
    "tagged_sentences = cess_esp.tagged_sents()\n",
    "#print('a random sentence: \\n-> {}'.format(random.choice(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCW_ENdLJudC"
   },
   "source": [
    "### Extraemos los datos de la cantidad de oraciones a ser usadas y un ejemplo de una oracion presente en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2clQUNdtQug4",
    "outputId": "4182f009-50e2-40fb-fe42-bf52158c94f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')]\n",
      "Tagged sentences:  6030\n",
      "Tagged words: 192686\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "Tagged sentences:  57340\n",
      "Tagged words: 1161192\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(cess_esp.tagged_words()))\n",
    "\n",
    "print(tagged_sentences1[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences1))\n",
    "print(\"Tagged words:\", len(brown.tagged_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_aFjuCQKG1O"
   },
   "source": [
    "### Se procede a Dividir en una lista de Oraciones dividida en lista de palabras y cada palabra con un correspondiente tag en un alista diferente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "516a_v5vQuhC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "sentences, tagss = [], []\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    tagss.append(np.array(tags))\n",
    "    \n",
    "with open(\"sentences.txt\", \"wb\") as fp:\n",
    "    pickle.dump(sentences, fp)\n",
    "\n",
    "with open(\"tags.txt\", \"wb\") as fp:\n",
    "    pickle.dump(tagss, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN_E4ePpKhvy"
   },
   "source": [
    "### Imprimimos una posicion de la lista como ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l6uGGSqZQuhM",
    "outputId": "f4addf11-656f-4a02-91c5-83828c445797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6030\n",
      "['La' 'alcaldesa' 'de' 'Málaga' 'y' 'cabeza' 'de' 'lista' 'del' 'PP' 'al'\n",
      " 'Congreso' 'por' 'esta' 'provincia' ',' 'Celia_Villalobos' ',' 'pidió'\n",
      " 'hoy' 'a' 'los' 'militantes' 'de' 'esta' 'formación' 'que' '*0*' 'sepan'\n",
      " '\"' 'administrar' 'la' 'victoria' '\"' ',' 'porque' '\"' 'no' 'vale' 'la'\n",
      " 'revancha' ',' 'el' 'insulto' 'o' 'el' 'ataque' ',' 'eso' 'es' 'para'\n",
      " 'ellos' '\"' '.']\n",
      "['da0fs0' 'ncfs000' 'sps00' 'np0000l' 'cc' 'ncfs000' 'sps00' 'ncfs000'\n",
      " 'spcms' 'np0000o' 'spcms' 'np0000o' 'sps00' 'dd0fs0' 'ncfs000' 'Fc'\n",
      " 'np0000p' 'Fc' 'vmis3s0' 'rg' 'sps00' 'da0mp0' 'nccp000' 'sps00' 'dd0fs0'\n",
      " 'ncfs000' 'cs' 'sn.e-SUJ' 'vmsp3p0' 'Fe' 'vmn0000' 'da0fs0' 'ncfs000'\n",
      " 'Fe' 'Fc' 'cs' 'Fe' 'rn' 'vmip3s0' 'da0fs0' 'ncfs000' 'Fc' 'da0ms0'\n",
      " 'ncms000' 'cc' 'da0ms0' 'ncms000' 'Fc' 'pd0ns000' 'vsip3s0' 'sps00'\n",
      " 'pp3mp000' 'Fe' 'Fp']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[6])\n",
    "print(tagss[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFQrAblFQuhT"
   },
   "source": [
    "### Dividimos el corpus de la siguiente manera, Utilizamos aproximadamente el 60% de las oraciones etiquetadas para el entrenamiento, el 20% como conjunto de validación y el 20% para evaluar nuestro modelo. Con esto se asegura que nunca  habrá oraciones repetidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZtLulrEOQuhU"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "(training_sentences, \n",
    " test_sentences, \n",
    " training_tags, \n",
    " test_tags) = train_test_split(sentences, tagss, test_size=0.2)\n",
    "\n",
    "(train_sentences, \n",
    " eval_sentences, \n",
    " train_tags, \n",
    " eval_tags) = train_test_split(training_sentences, training_tags, test_size=0.25)\n",
    "\n",
    "\n",
    "with open(\"train_sentences.txt\", \"wb\") as fp:\n",
    "    pickle.dump(train_sentences, fp)\n",
    "\n",
    "with open(\"eval_sentences.txt\", \"wb\") as fp:\n",
    "    pickle.dump(tagss, fp)\n",
    "\n",
    "with open(\"test_sentences.txt\", \"wb\") as fp:\n",
    "    pickle.dump(eval_sentences, fp)\n",
    "\n",
    "with open(\"train_tags.txt\", \"wb\") as fp:\n",
    "    pickle.dump(train_tags, fp)\n",
    "\n",
    "with open(\"eval_tags.txt\", \"wb\") as fp:\n",
    "    pickle.dump(eval_tags, fp)\n",
    "\n",
    "with open(\"test_tags.txt\", \"wb\") as fp:\n",
    "    pickle.dump(test_tags, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Mk0scnsK1OE"
   },
   "source": [
    "### Imprimimos los tamaños de las listas que nos indicaran el tamaño de filas de las matrices con las que estaremos trabajando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7HkjbP_IQuhZ",
    "outputId": "239b3106-9172-4dae-99ee-6400d2c9333e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_sentences:4824\n",
      "train_sentences: 3618\n",
      "test_sentences: 1206\n",
      "eval_sentences: 1206\n",
      "\n",
      "['*' 'El' 'Madrid' 'precisa' 'que' 'el' 'Deportivo' 'gane' 'la' 'Liga' ','\n",
      " 'porque' 'los' 'gallegos' 'no' 'son' 'considerados' 'unos' 'herederos'\n",
      " ',' 'sino' 'unos' 'entrometidos' 'que' 'se' 'supone' 'temporales' ','\n",
      " 'que' 'pertenecen' 'a' 'la' 'actualidad' 'más' 'rabiosa' 'y' 'no' 'a'\n",
      " 'la' 'historia' 'más' 'enrabietada' '.']\n",
      "['El' 'técnico' 'barcelonista' 'ha' 'asegurado' 'que' 'la' 'visita' 'de'\n",
      " 'Gaspart' 'ha' 'contribuido' 'a' '\"' 'sumar' '\"' ',' 'y' '*0*' 'ha'\n",
      " 'argumentado' 'que' 'el' 'encuentro' 'con' 'el' 'presidente' 'significa'\n",
      " 'que' 'en' 'el' 'Barcelona' '\"' 'todos' 'van' 'en' 'la' 'misma'\n",
      " 'dirección' '\"' '.']\n",
      "['Lo_suyo' ',' 'lo' 'de' 'las' 'ratas' ',' 'no' 'es' 'la' 'carroña' 'pura'\n",
      " 'y' 'dura' 'sino' 'la' 'vida' 'regalada' ',' 'el' 'eterno' 'banquete'\n",
      " 'de' 'sobras' 'y' 'residuos' ',' 'el' 'festín' 'organizado' 'a' 'la'\n",
      " 'sobra' 'de' 'la' 'abundancia' 'y' 'el' 'hartazgo' '.']\n",
      "\n",
      "training_tags:4824\n",
      "train_tags: 3618\n",
      "test_tags: 1206\n",
      "eval_tags: 1206\n",
      "\n",
      "['Fz' 'da0ms0' 'np0000l' 'vmip3s0' 'cs' 'da0ms0' 'np0000o' 'vmsp3s0'\n",
      " 'da0fs0' 'np0000a' 'Fc' 'cs' 'da0mp0' 'ncmp000' 'rn' 'vsip3p0' 'vmp00pm'\n",
      " 'di0mp0' 'ncmp000' 'Fc' 'cc' 'di0mp0' 'ncmp000' 'pr0cn000' 'p0000000'\n",
      " 'vmip3s0' 'aq0cp0' 'Fc' 'pr0cn000' 'vmip3p0' 'sps00' 'da0fs0' 'ncfs000'\n",
      " 'rg' 'aq0fs0' 'cc' 'rn' 'sps00' 'da0fs0' 'ncfs000' 'rg' 'aq0fsp' 'Fp']\n",
      "['da0ms0' 'ncms000' 'aq0cs0' 'vaip3s0' 'vmp00sm' 'cs' 'da0fs0' 'ncfs000'\n",
      " 'sps00' 'np00000' 'vaip3s0' 'vmp00sm' 'sps00' 'Fe' 'vmn0000' 'Fe' 'Fc'\n",
      " 'cc' 'sn.e-SUJ' 'vaip3s0' 'vmp00sm' 'cs' 'da0ms0' 'ncms000' 'sps00'\n",
      " 'da0ms0' 'ncms000' 'vmip3s0' 'cs' 'sps00' 'da0ms0' 'np00000' 'Fe'\n",
      " 'pi0mp000' 'vmip3p0' 'sps00' 'da0fs0' 'di0fs0' 'ncfs000' 'Fe' 'Fp']\n",
      "['px3ns000' 'Fc' 'da0ns0' 'sps00' 'da0fp0' 'ncfp000' 'Fc' 'rn' 'vsip3s0'\n",
      " 'da0fs0' 'ncfs000' 'aq0fs0' 'cc' 'aq0fs0' 'cc' 'da0fs0' 'ncfs000'\n",
      " 'aq0fsp' 'Fc' 'da0ms0' 'aq0ms0' 'ncms000' 'sps00' 'ncfp000' 'cc'\n",
      " 'ncmp000' 'Fc' 'da0ms0' 'ncms000' 'aq0msp' 'sps00' 'da0fs0' 'ncfs000'\n",
      " 'sps00' 'da0fs0' 'ncfs000' 'cc' 'da0ms0' 'ncms000' 'Fp']\n"
     ]
    }
   ],
   "source": [
    "print(\"training_sentences:\" + str(len(training_sentences)))\n",
    "print(\"train_sentences: \" + str(len(train_sentences)))\n",
    "print(\"test_sentences: \" + str(len(test_sentences)))\n",
    "print(\"eval_sentences: \" + str(len(eval_sentences)) + \"\\n\")\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(test_sentences[0])\n",
    "print(eval_sentences[0])\n",
    "\n",
    "print(\"\\ntraining_tags:\" + str(len(training_sentences)))\n",
    "print(\"train_tags: \" + str(len(train_tags)))\n",
    "print(\"test_tags: \" + str(len(test_tags)))\n",
    "print(\"eval_tags: \" + str(len(eval_tags)) + \"\\n\")\n",
    "\n",
    "print(train_tags[0])\n",
    "print(test_tags[0])\n",
    "print(eval_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd-i6q85Quho"
   },
   "source": [
    "### Ahora creamos una array con todas las palabras y los tags presentes en el corpus, adicionalmente se crea un diccionario que contiene las palabras unicas y los tags unicos de tal forma que no se repitan y que contienen un indice o llave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qdCNulCoQuhr",
    "outputId": "98ef44f4-7d36-440d-eefd-f3571040f747"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24499\n",
      "291\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "words, tagsss = set([]), set([])\n",
    " \n",
    "for s in (train_sentences + eval_sentences + test_sentences):\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "\n",
    "for ts in (train_tags + eval_tags + test_tags):\n",
    "    for t in ts:\n",
    "        tagsss.add(t)\n",
    "\n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 2 for i, t in enumerate(list(tagsss))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding\n",
    "tag2index['-OOV-'] = 1  # The special value used to padding\n",
    "\n",
    "print (len(word2index))\n",
    "print (len(tag2index))\n",
    "\n",
    "np.save('word2index.npy', word2index)\n",
    "np.save('tag2index.npy', tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEA9Ek-GOYOn"
   },
   "source": [
    "### Ahora procedemos a transformar cada uno de los conjuntos de oraciones y tags en vectores numericos, modificando la palabra o tag en un Valor numerico que corresponde a una llave en el diccionario de palabras o tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "69eec13kQuh2"
   },
   "outputs": [],
   "source": [
    "train_sentences_X, eval_sentences_X, test_sentences_X, train_tags_y, eval_tags_y, test_tags_y = [], [], [], [], [], []\n",
    "\n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "\n",
    "for s in eval_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    eval_sentences_X.append(s_int)\n",
    "\n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "for s in train_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    train_tags_y.append(s_int)\n",
    "\n",
    "for s in eval_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    eval_tags_y.append(s_int)\n",
    "\n",
    "for s in test_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    test_tags_y.append(s_int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_lXW1mBPNkf"
   },
   "source": [
    "### Se imprime la longitud de las matrices y una muestra de cada una de las matrices creadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Y5A4d_dzQuh6",
    "outputId": "a87e29fe-80b1-4e0c-bb1d-0655cd6e1d73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitudes de las Matrices:\n",
      "3618\n",
      "1206\n",
      "1206\n",
      "3618\n",
      "1206\n",
      "1206\n",
      "\n",
      "Muestra de Datos presentes en las Matrices con las transformaciones:\n",
      "\n",
      "[10717, 19905, 17168, 14314, 6299, 19905, 42, 10497, 8116, 23263, 11555, 7762, 4562, 7609, 22154, 14754, 12636, 9076, 15585, 11555, 22375, 9076, 5187, 6299, 881, 6919, 23979, 11555, 6299, 14141, 4120, 8116, 13902, 9639, 427, 14263, 22154, 4120, 8116, 11579, 9639, 21769, 239]\n",
      "[16211, 11555, 1828, 14251, 16815, 6244, 11555, 22154, 11342, 8116, 6106, 20916, 14263, 15977, 22375, 8116, 9544, 8545, 11555, 19905, 19146, 664, 14251, 21977, 14263, 6308, 11555, 19905, 4569, 18814, 4120, 8116, 6298, 14251, 8116, 14196, 14263, 19905, 19098, 239]\n",
      "[19905, 6032, 7161, 19717, 22553, 6299, 8116, 5859, 14251, 10876, 19717, 21111, 4120, 15330, 16891, 15330, 11555, 14263, 8139, 19717, 7101, 6299, 19905, 1305, 8235, 19905, 15731, 6414, 6299, 8835, 19905, 22788, 15330, 250, 11452, 8835, 8116, 1463, 13313, 15330, 239]\n",
      "[31, 153, 93, 91, 86, 153, 95, 276, 238, 242, 76, 86, 278, 73, 125, 96, 250, 97, 73, 76, 162, 97, 73, 52, 139, 91, 90, 76, 52, 275, 38, 238, 277, 284, 164, 162, 125, 38, 238, 277, 284, 263, 258]\n",
      "[166, 76, 177, 38, 142, 268, 76, 125, 261, 238, 277, 164, 162, 164, 162, 238, 277, 263, 76, 153, 283, 288, 38, 268, 162, 73, 76, 153, 288, 219, 38, 238, 277, 38, 238, 277, 162, 153, 288, 258]\n",
      "[153, 288, 123, 68, 227, 86, 238, 277, 38, 273, 68, 227, 38, 105, 226, 105, 76, 162, 148, 68, 227, 86, 153, 288, 38, 153, 288, 91, 86, 38, 153, 273, 105, 257, 275, 38, 238, 207, 277, 105, 258]\n"
     ]
    }
   ],
   "source": [
    "print(\"Longitudes de las Matrices:\")\n",
    "print(len(train_sentences_X))\n",
    "print(len(eval_sentences_X))\n",
    "print(len(test_sentences_X))\n",
    "print(len(train_tags_y))\n",
    "print(len(eval_tags_y))\n",
    "print(len(test_tags_y))\n",
    "\n",
    "print(\"\\nMuestra de Datos presentes en las Matrices con las transformaciones:\\n\")\n",
    "\n",
    "print(train_sentences_X[0])\n",
    "print(eval_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(eval_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWLspkzfQ513"
   },
   "source": [
    "### Se calcula cual es la oracion que mayor cantidad de Palabras contiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Gif6KsESQuh_",
    "outputId": "a0be4dd9-13fc-473f-c608-7334de984a52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH1 = len(max(train_sentences_X, key=len))\n",
    "MAX_LENGTH2 = len(max(eval_sentences_X, key=len))\n",
    "MAX_LENGTH3 = len(max(test_sentences_X, key=len))\n",
    "\n",
    "l = [MAX_LENGTH1, MAX_LENGTH2, MAX_LENGTH3]\n",
    "MAX_LENGTH = max(l)\n",
    "\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4ffWaDqRA1_"
   },
   "source": [
    "### Se procede a Normalizar las matrices para que todas contengan el mismo numero de columans, con la longitud maxima de palabras encontradas anteriormente, esto se logra agregando ceros a la derecha en las posiciones que hacen falta en el vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mn7iuMIOQuiI",
    "outputId": "8a6f3032-ab8b-426c-f45a-d182bd5e7cb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10717 19905 17168 14314  6299 19905    42 10497  8116 23263 11555  7762\n",
      "  4562  7609 22154 14754 12636  9076 15585 11555 22375  9076  5187  6299\n",
      "   881  6919 23979 11555  6299 14141  4120  8116 13902  9639   427 14263\n",
      " 22154  4120  8116 11579  9639 21769   239     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "[ 31 153  93  91  86 153  95 276 238 242  76  86 278  73 125  96 250  97\n",
      "  73  76 162  97  73  52 139  91  90  76  52 275  38 238 277 284 164 162\n",
      " 125  38 238 277 284 263 258   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n",
      "[166  76 177  38 142 268  76 125 261 238 277 164 162 164 162 238 277 263\n",
      "  76 153 283 288  38 268 162  73  76 153 288 219  38 238 277  38 238 277\n",
      " 162 153 288 258   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n",
      "[153 288 123  68 227  86 238 277  38 273  68 227  38 105 226 105  76 162\n",
      " 148  68 227  86 153 288  38 153 288  91  86  38 153 273 105 257 275  38\n",
      " 238 207 277 105 258   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "eval_sentences_X = pad_sequences(eval_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "eval_tags_y = pad_sequences(eval_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "#print(eval_sentences_X[0])\n",
    "#print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(eval_tags_y[0])\n",
    "print(test_tags_y[0])\n",
    "\n",
    "np.save('train_sentences_X.npy', train_sentences_X)\n",
    "np.save('eval_sentences_X.npy', eval_sentences_X)\n",
    "np.save('test_sentences_X.npy', test_sentences_X)\n",
    "np.save('train_tags_y.npy', train_tags_y)\n",
    "np.save('eval_tags_y.npy', eval_tags_y)\n",
    "np.save('test_tags_y.npy', test_tags_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elkKsVbBNrYO"
   },
   "source": [
    "### Definimos la funcion con la cual categorizaremos los tags y los covertiremos un vector One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qGw5_dPX5xc0"
   },
   "outputs": [],
   "source": [
    "def to_categoricals(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def encode(data):\n",
    "    print('Shape of data (BEFORE encode): %s' % str(data.shape))\n",
    "    encoded = to_categorical(data)\n",
    "    print('Shape of data (AFTER  encode): %s\\n' % str(encoded.shape))\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOmqn-5ZNg23"
   },
   "source": [
    "### Desarrollamos una prueba de la categorización de los tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "3618\n",
      "1206\n"
     ]
    }
   ],
   "source": [
    "cat_train_tags_y = to_categoricals(train_tags_y, len(tag2index))\n",
    "cat_eval_tags_y  = to_categoricals(eval_tags_y, len(tag2index))\n",
    "cat_test_tags_y  = to_categoricals(test_tags_y, len(tag2index))\n",
    "\n",
    "print(cat_train_tags_y[0])\n",
    "print(len(cat_train_tags_y))\n",
    "print(len(cat_test_tags_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURES DEL SEGUNDO MODELO\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    \n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "57283\n",
      "[{'word': 'The', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'T', 'prefix-2': 'Th', 'prefix-3': 'The', 'suffix-1': 'e', 'suffix-2': 'he', 'suffix-3': 'The', 'prev_word': '', 'next_word': 'Fulton', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'Fulton', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'F', 'prefix-2': 'Fu', 'prefix-3': 'Ful', 'suffix-1': 'n', 'suffix-2': 'on', 'suffix-3': 'ton', 'prev_word': 'The', 'next_word': 'County', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'County', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'C', 'prefix-2': 'Co', 'prefix-3': 'Cou', 'suffix-1': 'y', 'suffix-2': 'ty', 'suffix-3': 'nty', 'prev_word': 'Fulton', 'next_word': 'Grand', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'Grand', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'G', 'prefix-2': 'Gr', 'prefix-3': 'Gra', 'suffix-1': 'd', 'suffix-2': 'nd', 'suffix-3': 'and', 'prev_word': 'County', 'next_word': 'Jury', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'Jury', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'J', 'prefix-2': 'Ju', 'prefix-3': 'Jur', 'suffix-1': 'y', 'suffix-2': 'ry', 'suffix-3': 'ury', 'prev_word': 'Grand', 'next_word': 'said', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'said', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 's', 'prefix-2': 'sa', 'prefix-3': 'sai', 'suffix-1': 'd', 'suffix-2': 'id', 'suffix-3': 'aid', 'prev_word': 'Jury', 'next_word': 'Friday', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'Friday', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'F', 'prefix-2': 'Fr', 'prefix-3': 'Fri', 'suffix-1': 'y', 'suffix-2': 'ay', 'suffix-3': 'day', 'prev_word': 'said', 'next_word': 'an', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'an', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'a', 'prefix-2': 'an', 'prefix-3': 'an', 'suffix-1': 'n', 'suffix-2': 'an', 'suffix-3': 'an', 'prev_word': 'Friday', 'next_word': 'investigation', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'investigation', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'i', 'prefix-2': 'in', 'prefix-3': 'inv', 'suffix-1': 'n', 'suffix-2': 'on', 'suffix-3': 'ion', 'prev_word': 'an', 'next_word': 'of', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'of', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'o', 'prefix-2': 'of', 'prefix-3': 'of', 'suffix-1': 'f', 'suffix-2': 'of', 'suffix-3': 'of', 'prev_word': 'investigation', 'next_word': \"Atlanta's\", 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': \"Atlanta's\", 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'A', 'prefix-2': 'At', 'prefix-3': 'Atl', 'suffix-1': 's', 'suffix-2': \"'s\", 'suffix-3': \"a's\", 'prev_word': 'of', 'next_word': 'recent', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'recent', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'r', 'prefix-2': 're', 'prefix-3': 'rec', 'suffix-1': 't', 'suffix-2': 'nt', 'suffix-3': 'ent', 'prev_word': \"Atlanta's\", 'next_word': 'primary', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'primary', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'p', 'prefix-2': 'pr', 'prefix-3': 'pri', 'suffix-1': 'y', 'suffix-2': 'ry', 'suffix-3': 'ary', 'prev_word': 'recent', 'next_word': 'election', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'election', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'e', 'prefix-2': 'el', 'prefix-3': 'ele', 'suffix-1': 'n', 'suffix-2': 'on', 'suffix-3': 'ion', 'prev_word': 'primary', 'next_word': 'produced', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'produced', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'p', 'prefix-2': 'pr', 'prefix-3': 'pro', 'suffix-1': 'd', 'suffix-2': 'ed', 'suffix-3': 'ced', 'prev_word': 'election', 'next_word': '``', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': '``', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': '`', 'prefix-2': '``', 'prefix-3': '``', 'suffix-1': '`', 'suffix-2': '``', 'suffix-3': '``', 'prev_word': 'produced', 'next_word': 'no', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'no', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'n', 'prefix-2': 'no', 'prefix-3': 'no', 'suffix-1': 'o', 'suffix-2': 'no', 'suffix-3': 'no', 'prev_word': '``', 'next_word': 'evidence', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'evidence', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'e', 'prefix-2': 'ev', 'prefix-3': 'evi', 'suffix-1': 'e', 'suffix-2': 'ce', 'suffix-3': 'nce', 'prev_word': 'no', 'next_word': \"''\", 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': \"''\", 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': \"'\", 'prefix-2': \"''\", 'prefix-3': \"''\", 'suffix-1': \"'\", 'suffix-2': \"''\", 'suffix-3': \"''\", 'prev_word': 'evidence', 'next_word': 'that', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'that', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 't', 'prefix-2': 'th', 'prefix-3': 'tha', 'suffix-1': 't', 'suffix-2': 'at', 'suffix-3': 'hat', 'prev_word': \"''\", 'next_word': 'any', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'any', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'a', 'prefix-2': 'an', 'prefix-3': 'any', 'suffix-1': 'y', 'suffix-2': 'ny', 'suffix-3': 'any', 'prev_word': 'that', 'next_word': 'irregularities', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'irregularities', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'i', 'prefix-2': 'ir', 'prefix-3': 'irr', 'suffix-1': 's', 'suffix-2': 'es', 'suffix-3': 'ies', 'prev_word': 'any', 'next_word': 'took', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'took', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 't', 'prefix-2': 'to', 'prefix-3': 'too', 'suffix-1': 'k', 'suffix-2': 'ok', 'suffix-3': 'ook', 'prev_word': 'irregularities', 'next_word': 'place', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'place', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'p', 'prefix-2': 'pl', 'prefix-3': 'pla', 'suffix-1': 'e', 'suffix-2': 'ce', 'suffix-3': 'ace', 'prev_word': 'took', 'next_word': '.', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': '.', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': '.', 'prefix-2': '.', 'prefix-3': '.', 'suffix-1': '.', 'suffix-2': '.', 'suffix-3': '.', 'prev_word': 'place', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}]\n",
      "['AT', 'NP-TL', 'NN-TL', 'JJ-TL', 'NN-TL', 'VBD', 'NR', 'AT', 'NN', 'IN', 'NP$', 'JJ', 'NN', 'NN', 'VBD', '``', 'AT', 'NN', \"''\", 'CS', 'DTI', 'NNS', 'VBD', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "#DIVISION DEL SEGUNDO CORPUS\n",
    "from nltk.tag.util import untag\n",
    "\n",
    "\n",
    "cutoff1 = int(.001 * len(tagged_sentences1))\n",
    "training_sentences1 = tagged_sentences1[:cutoff1]\n",
    "test_sentences1 = tagged_sentences1[cutoff1:]\n",
    " \n",
    "def transform_to_dataset(tagged_sentences1):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences1:\n",
    "        X.append([features(untag(tagged), index) for index in range(len(tagged))])\n",
    "        y.append([tag for _, tag in tagged])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "X_train1, y_train1 = transform_to_dataset(training_sentences1)\n",
    "X_test1, y_test1 = transform_to_dataset(test_sentences1)\n",
    " \n",
    "print(len(X_train1))     \n",
    "print(len(X_test1))         \n",
    "print(X_train1[0])\n",
    "print(y_train1[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn-crfsuite in /home/daniel/.local/lib/python3.7/site-packages (0.3.6)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in /home/daniel/.local/lib/python3.7/site-packages (from sklearn-crfsuite) (0.9.7)\n",
      "Requirement already satisfied: tqdm>=2.0 in /home/daniel/.local/lib/python3.7/site-packages (from sklearn-crfsuite) (4.46.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sklearn-crfsuite) (1.12.0)\n",
      "Requirement already satisfied: tabulate in /home/daniel/.local/lib/python3.7/site-packages (from sklearn-crfsuite) (0.8.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-_gAQ7qrWTQ"
   },
   "source": [
    "## PARTE 2  -  Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ORyC-422jaD9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6980955582265169704\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 3991473579146994551\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "## Funcion que permite forzar el uso de GPU cuando estan presentes\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odDOhtO4NZDd"
   },
   "source": [
    "### Definimos el Modelo Base con el cual se procedera a desarrollar la fase de Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo base tiene como entrada una oración de límite máximo de 149, la matriz de embedding  que inicia con el entrenamiento de la red se comporta como una matriz tridimensional de 24500 X 149 X 128, los 128 porque esa fue la ventana de entrenamiento definida. En cada EPOCH  la matriz de embedding se llena con los pesos que se propagan después de calculado el error. Viene una capa que se considera oculta, aplicando la función de máximos, RELU. Después viene la distribución de probabilidad  asignada a los tags que se asignan a la oración de entrada de las 3618 del corpus de entrenamiento. Finalmente, la salida de la distibución se discretiza o se comprime con la función softmax de manera que permita que los valores de distibución quden en el rango [0,1]. la función de pérdida (categorical_crossentropy) se calcula la actualización de los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "x31rRt8PQuiW",
    "outputId": "eb0c9647-849c-4697-b151-802f88de7e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-9pt02igz\n",
      "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-9pt02igz\n",
      "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /home/daniel/.local/lib/python3.7/site-packages\n",
      "Requirement already satisfied: keras in /home/daniel/.local/lib/python3.7/site-packages (from keras-contrib==2.0.8) (2.3.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/daniel/.local/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/daniel/.local/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (1.17.0)\n",
      "Requirement already satisfied: h5py in /home/daniel/.local/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/daniel/.local/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/daniel/.local/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
      "Building wheels for collected packages: keras-contrib\n",
      "  Building wheel for keras-contrib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101065 sha256=ae5395cbc35078b740f2dba31ad103fc47c26371f05c9f80efb5eafbfae3dd37\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xww75sal/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n",
      "Successfully built keras-contrib\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/home/daniel/.local/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 149)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 149, 128)          3135872   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 149, 256)          263168    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 149, 256)          525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 149, 291)          74787     \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 149, 291)          170235    \n",
      "=================================================================\n",
      "Total params: 4,169,374\n",
      "Trainable params: 4,169,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip3 install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "import keras as k\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "input = Input(shape=(MAX_LENGTH,))\n",
    "word_embedding_size = 128\n",
    "\n",
    "# Embedding Layer\n",
    "model = Embedding(input_dim=len(word2index), output_dim=word_embedding_size, input_length=MAX_LENGTH)(input)\n",
    "\n",
    "# BI-LSTM Layer\n",
    "model = Bidirectional(LSTM(units=word_embedding_size, \n",
    "                           return_sequences=True, \n",
    "                           dropout=0.5, \n",
    "                           recurrent_dropout=0.5, \n",
    "                           kernel_initializer=k.initializers.he_normal()))(model)\n",
    "model = LSTM(units=word_embedding_size * 2, \n",
    "             return_sequences=True, \n",
    "             dropout=0.5, \n",
    "             recurrent_dropout=0.5, \n",
    "             kernel_initializer=k.initializers.he_normal())(model)\n",
    "\n",
    "# TimeDistributed Layer\n",
    "model = TimeDistributed(Dense(len(tag2index), activation=\"relu\"))(model)  \n",
    "\n",
    "# CRF Layer\n",
    "crf = CRF(len(tag2index))\n",
    "\n",
    "out = crf(model)  # output\n",
    "model = Model(input, out)\n",
    "\n",
    "\n",
    "#Optimiser \n",
    "adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XghotI4NG9G"
   },
   "source": [
    "### Se desarrolla el entrenamiento del modelo, este es el alimentador de la red neuronal, aquíne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "C0gOhZznbg6V",
    "outputId": "cfb9376e-230d-40ea-ffb8-ace34c8234b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nmodel_hist = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=20, \\n          validation_data=(eval_sentences_X, cat_eval_tags_y))\\n\\n# serialize model to JSON\\nmodel_json = model.to_json()\\nwith open(\"mb-00.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n\\n# serialize weights to HDF5\\nmodel.save_weights(\"mb-00.h5\")\\nprint(\"Saved model to disk\")\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sudo pip install h5py\n",
    "\n",
    "import os\n",
    "model_hist = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=20, \n",
    "          validation_data=(eval_sentences_X, cat_eval_tags_y))\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"mb-00.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"mb-00.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el modelo que se entreno en el paso anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 149)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 149, 128)          3135872   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 149, 256)          263168    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 149, 256)          525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 149, 291)          74787     \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 149, 291)          170235    \n",
      "=================================================================\n",
      "Total params: 4,169,374\n",
      "Trainable params: 4,169,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from keras.models import model_from_json\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "\n",
    "\n",
    "custom_objects={'CRF': CRF,'crf_loss': crf.loss_function,'crf_viterbi_accuracy':crf.accuracy}\n",
    "# load json and create model\n",
    "json_file = open('mb-00.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json, custom_objects = custom_objects)\n",
    "# load weights into new model\n",
    "model.load_weights(\"mb-00.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# Compile model\n",
    "model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
    "model.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINICION DEL SEGUNDO MODELO\n",
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "model1 = CRF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.7/site-packages/sklearn/base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(keep_tempfiles=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ENTRENAMIENTO DEL SEGUNDO MODELO\n",
    "model1.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTRENAMIENTO DEL SEGUNDO MODELO\n",
    "def pos_tag1(sentence):\n",
    "    sentence_features = [features(sentence, index) for index in range(len(sentence))]\n",
    "    return list(zip(sentence, model1.predict([sentence_features])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hTDgQb2rWTa"
   },
   "source": [
    "## PARTE 3  -  Evaluación de los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6912120976989197\n"
     ]
    }
   ],
   "source": [
    "#EVALUACION DEL SEGUNDO MODELO\n",
    "from sklearn_crfsuite import metrics\n",
    " \n",
    "y_pred1 = model1.predict(X_test1)\n",
    "print(metrics.flat_accuracy_score(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdSkk8mzM1KN"
   },
   "source": [
    "### Evaluamos el modelo y calculamos el valor de precision con respecto a los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "cD-YI5Fgb3Kt",
    "outputId": "98cb9946-4f38-4176-c5ad-5932d07962b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206/1206 [==============================] - 416s 345ms/step\n",
      "crf_viterbi_accuracy: 77.75596380233765\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, cat_test_tags_y)\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # acc: 97.66269326210022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAhkgtWHQuij"
   },
   "source": [
    "### Definimos la funcion que nos servira para graficar el comportamiento del modelo en cada epoca del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "JaBUkInNQuik"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_performance(train_loss, train_acc, train_val_loss, train_val_acc):\n",
    "    \"\"\" Plot model loss and accuracy through epochs. \"\"\"\n",
    "    blue= '#34495E'\n",
    "    green = '#2ECC71'\n",
    "    orange = '#E23B13'\n",
    "    \n",
    "    # plot model loss\n",
    "    fig, (ax1, ax2) = plt.subplots(2, figsize=(10, 8))\n",
    "    ax1.plot(range(1, len(train_loss) + 1), train_loss, blue, linewidth=5, label='training')\n",
    "    ax1.plot(range(1, len(train_val_loss) + 1), train_val_loss, green, linewidth=5, label='validation')\n",
    "    ax1.set_xlabel('# epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.tick_params('y')\n",
    "    ax1.legend(loc='upper right', shadow=False)\n",
    "    ax1.set_title('Model loss through #epochs', color=orange, fontweight='bold')\n",
    "    \n",
    "    # plot model accuracy\n",
    "    ax2.plot(range(1, len(train_acc) + 1), train_acc, blue, linewidth=5, label='training')\n",
    "    ax2.plot(range(1, len(train_val_acc) + 1), train_val_acc, green, linewidth=5, label='validation')\n",
    "    ax2.set_xlabel('# epoch')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.tick_params('y')\n",
    "    ax2.legend(loc='lower right', shadow=False)\n",
    "    ax2.set_title('Model accuracy through #epochs', color=orange, fontweight='bold')\n",
    "    \n",
    "    fig.savefig('training-mb-00.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxQh1AtuQuis"
   },
   "source": [
    "### Procedemos a Graficar el comportamiento del Entrenamiento, tanto del conjunto de entrenamiento como el de validación con respecto a la cantidad de epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Gs5f3U1nQuit",
    "outputId": "4c4ad746-3bbc-4325-9356-8045506b33a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplot_model_performance(\\n    train_loss=model_hist.history.get('loss', []),\\n    train_acc=model_hist.history.get('acc', []),\\n    train_val_loss=model_hist.history.get('val_loss', []),\\n    train_val_acc=model_hist.history.get('val_acc', [])\\n)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plot_model_performance(\n",
    "    train_loss=model_hist.history.get('loss', []),\n",
    "    train_acc=model_hist.history.get('acc', []),\n",
    "    train_val_loss=model_hist.history.get('val_loss', []),\n",
    "    train_val_acc=model_hist.history.get('val_acc', [])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqiuy8q4GYjF"
   },
   "source": [
    "### Función que Permite convertir Indices en Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "YJ6GaLot9yZR"
   },
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-1GH3ZYuLc-"
   },
   "source": [
    "### Hacemos la prediccion sobre el conjunto de pruebas. De la distribución probabilítica a etiquetas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6HgbDqqsR4a7",
    "outputId": "63275f95-19ca-41f0-db78-e79ece5e6317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vaip2s0', 'aq0ms0', 'sn.e-SUJ', 'sn.e-CD', 'pi0ms000', 'vmp00sf', 'sn.e-CD', 'sn.e-CD', 'sn.e-CD', 'sn.e-CD', 'sn.e-CD', 'vmp00sf', 'dd0cs0', 'vsn0000', 'vmif3p0', 'vmif3p0', 'vmif3p0', 'pt0mp000', 'sn.e-CD', 'sn.e-CD', 'vmif3p0', 'pp3mpa00', 'vmp00sf', 'sn.e-CD', 'nccp000', 'vmp00sf', 'aq0ms0', 'dp1css', 'vmii2s0', 'vmif3p0', 'vaip2s0', 'vmic3s0', 'vmif3p0', 'pp3mpa00', 'vmii2s0', 'vmif3p0', 'sn.e-CD', 'sn.e-CD', 'dd0cs0', 'vmif3p0', 'sn.e-SUJ', 'pp3fsa00', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prediction = model.predict(test_sentences_X)\n",
    "log_tokens = logits_to_tokens(prediction, {i: t for t, i in tag2index.items()})\n",
    "\n",
    "print(log_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT6IIQXrQuix"
   },
   "source": [
    "### Hallamos los valores de F1 score, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "GqTuNxppFNu-",
    "outputId": "ce0f608a-171e-411d-f5d1-88666b57cd19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/daniel/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "classification_report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Faa       0.00      0.00      0.00         2\n",
      "         Fat       0.00      0.00      0.00         5\n",
      "          Fc       0.00      0.00      0.00      2291\n",
      "          Fd       0.00      0.00      0.00        87\n",
      "          Fe       0.00      0.00      0.00       631\n",
      "          Fg       0.00      0.00      0.00       226\n",
      "          Fh       0.00      0.00      0.00         3\n",
      "         Fia       0.00      0.00      0.00         6\n",
      "         Fit       0.00      0.00      0.00        19\n",
      "          Fp       0.00      0.00      0.00      1178\n",
      "         Fpa       0.00      0.00      0.00       156\n",
      "         Fpt       0.00      0.00      0.00       160\n",
      "          Fs       0.00      0.00      0.00        13\n",
      "          Fx       0.00      0.00      0.00        41\n",
      "          Fz       0.00      0.00      0.00         2\n",
      "           W       0.00      0.00      0.00       194\n",
      "           Z       0.00      0.00      0.00       320\n",
      "          Zm       0.00      0.00      0.00        35\n",
      "          Zp       0.00      0.00      0.00        45\n",
      "      ao0fp0       0.00      0.00      0.00        17\n",
      "      ao0fs0       0.00      0.00      0.00        66\n",
      "      ao0mp0       0.00      0.00      0.00        23\n",
      "      ao0ms0       0.00      0.00      0.00        59\n",
      "     aq00000       0.00      0.00      0.00         2\n",
      "      aq0cn0       0.00      0.00      0.00         4\n",
      "      aq0cp0       0.00      0.00      0.00       228\n",
      "      aq0cs0       0.00      0.00      0.00       590\n",
      "      aq0fp0       0.00      0.00      0.00       128\n",
      "      aq0fpp       0.00      0.00      0.00        52\n",
      "      aq0fs0       0.00      0.00      0.00       355\n",
      "      aq0fsp       0.00      0.00      0.00       140\n",
      "      aq0mp0       0.00      0.00      0.00       202\n",
      "      aq0mpp       0.00      0.00      0.00        96\n",
      "      aq0ms0       0.01      0.03      0.02       491\n",
      "      aq0msp       0.00      0.00      0.00       222\n",
      "          cc       0.00      0.00      0.00      1222\n",
      "          cs       0.00      0.00      0.00       927\n",
      "      da0fp0       0.00      0.00      0.00       374\n",
      "      da0fs0       0.00      0.00      0.00      1324\n",
      "      da0mp0       0.00      0.00      0.00       637\n",
      "      da0ms0       0.00      0.00      0.00      1275\n",
      "      da0ns0       0.00      0.00      0.00       110\n",
      "      dd0cp0       0.00      0.00      0.00         2\n",
      "      dd0cs0       0.00      0.00      0.00         4\n",
      "      dd0fp0       0.00      0.00      0.00        18\n",
      "      dd0fs0       0.00      0.00      0.00        67\n",
      "      dd0mp0       0.00      0.00      0.00        30\n",
      "      dd0ms0       0.00      0.00      0.00       101\n",
      "      di0cp0       0.00      0.00      0.00         5\n",
      "      di0cs0       0.00      0.00      0.00        26\n",
      "      di0fp0       0.00      0.00      0.00        71\n",
      "      di0fs0       0.00      0.00      0.00       335\n",
      "      di0mp0       0.00      0.00      0.00       112\n",
      "      di0ms0       0.00      0.00      0.00       482\n",
      "      dn0cp0       0.00      0.00      0.00       149\n",
      "      dn0cs0       0.00      0.00      0.00         1\n",
      "      dn0fp0       0.00      0.00      0.00         4\n",
      "      dn0fs0       0.00      0.00      0.00         2\n",
      "      dn0mp0       0.00      0.00      0.00         5\n",
      "      dn0ms0       0.00      0.00      0.00         9\n",
      "      dp1cps       0.00      0.00      0.00         1\n",
      "      dp1css       0.00      0.00      0.00         8\n",
      "      dp1fpp       0.00      0.00      0.00         1\n",
      "      dp1fsp       0.00      0.00      0.00         6\n",
      "      dp1mpp       0.00      0.00      0.00         7\n",
      "      dp1msp       0.00      0.00      0.00         8\n",
      "      dp2css       0.00      0.00      0.00         3\n",
      "      dp3cp0       0.00      0.00      0.00       107\n",
      "      dp3cs0       0.01      0.12      0.03       275\n",
      "      dp3fs0       0.00      0.00      0.00         1\n",
      "      dt0cn0       0.00      0.00      0.00         3\n",
      "           i       0.00      0.00      0.00         4\n",
      "     nc00000       0.00      0.00      0.00        26\n",
      "     nccn000       0.00      0.00      0.00         5\n",
      "     nccp000       0.00      0.00      0.00       103\n",
      "     nccs000       0.00      0.00      0.00       146\n",
      "     ncfn000       0.00      0.00      0.00        15\n",
      "     ncfp000       0.00      0.00      0.00       788\n",
      "     ncfs000       0.00      0.00      0.00      2132\n",
      "     ncmn000       0.00      0.00      0.00        24\n",
      "     ncmp000       0.00      0.00      0.00      1166\n",
      "     ncms000       0.00      0.00      0.00      2365\n",
      "     np00000       0.00      0.00      0.00        51\n",
      "     np0000a       0.00      0.00      0.00       202\n",
      "     np0000l       0.00      0.00      0.00       412\n",
      "     np0000o       0.00      0.00      0.00       656\n",
      "     np0000p       0.00      0.00      0.00       720\n",
      "    p0000000       0.00      0.00      0.00       193\n",
      "    p010p000       0.00      0.00      0.00         2\n",
      "    p010s000       0.00      0.00      0.00         3\n",
      "    p020s000       0.00      0.00      0.00         1\n",
      "    p0300000       0.00      0.00      0.00       217\n",
      "    pd0fp000       0.00      0.00      0.00         3\n",
      "    pd0fs000       0.00      0.00      0.00         5\n",
      "    pd0mp000       0.00      0.00      0.00         6\n",
      "    pd0ms000       0.00      0.00      0.00         8\n",
      "    pd0ns000       0.00      0.00      0.00        22\n",
      "    pi0cp000       0.00      0.00      0.00         2\n",
      "    pi0cs000       0.00      0.00      0.00        40\n",
      "    pi0fp000       0.00      0.00      0.00         7\n",
      "    pi0fs000       0.00      0.00      0.00        11\n",
      "    pi0mp000       0.00      0.00      0.00        35\n",
      "    pi0ms000       0.00      0.00      0.00        54\n",
      "    pn0cp000       0.00      0.00      0.00        27\n",
      "    pn0mp000       0.00      0.00      0.00         3\n",
      "    pn0ms000       0.00      0.00      0.00         2\n",
      "    pp1cp000       0.00      0.00      0.00        27\n",
      "    pp1cs000       0.00      0.00      0.00        27\n",
      "    pp1csn00       0.00      0.00      0.00        16\n",
      "    pp1cso00       0.00      0.00      0.00         5\n",
      "    pp1mp000       0.00      0.00      0.00         9\n",
      "    pp2cs000       0.00      0.00      0.00         4\n",
      "    pp2cs00p       0.00      0.00      0.00         4\n",
      "    pp2csn00       0.00      0.00      0.00         2\n",
      "    pp2cso00       0.00      0.00      0.00         1\n",
      "    pp3cn000       0.00      0.00      0.00        10\n",
      "    pp3cna00       0.00      0.00      0.00         2\n",
      "    pp3cno00       0.00      0.00      0.00         5\n",
      "    pp3cpa00       0.00      0.00      0.00         2\n",
      "    pp3cpd00       0.00      0.00      0.00        11\n",
      "    pp3csa00       0.00      0.00      0.00         1\n",
      "    pp3csd00       0.00      0.00      0.00        63\n",
      "    pp3fp000       0.00      0.00      0.00         6\n",
      "    pp3fpa00       0.00      0.00      0.00        10\n",
      "    pp3fs000       0.00      0.00      0.00        13\n",
      "    pp3fsa00       0.00      0.00      0.00        17\n",
      "    pp3mp000       0.00      0.00      0.00        26\n",
      "    pp3mpa00       0.00      0.00      0.00         4\n",
      "    pp3ms000       0.00      0.00      0.00        23\n",
      "    pp3msa00       0.00      0.00      0.00        33\n",
      "    pp3ns000       0.00      0.00      0.00        15\n",
      "    pr000000       0.00      0.00      0.00        26\n",
      "    pr0cn000       0.00      0.00      0.00       616\n",
      "    pr0cp000       0.00      0.00      0.00        10\n",
      "    pr0cs000       0.00      0.00      0.00        32\n",
      "    pr0fp000       0.00      0.00      0.00         1\n",
      "    pr0fs000       0.00      0.00      0.00         5\n",
      "    pr0ms000       0.00      0.00      0.00         3\n",
      "    pt000000       0.00      0.00      0.00        12\n",
      "    pt0cp000       0.00      0.00      0.00         1\n",
      "    pt0cs000       0.00      0.00      0.00        18\n",
      "    pt0mp000       0.00      0.00      0.00         2\n",
      "    px1fs0p0       0.00      0.00      0.00         1\n",
      "    px3ns000       0.00      0.00      0.00         1\n",
      "          rg       0.00      0.00      0.00      1208\n",
      "          rn       0.00      0.00      0.00       265\n",
      "      sn-SUJ       0.00      0.00      0.00         1\n",
      "   sn.co-SUJ       0.00      0.00      0.00         0\n",
      "        sn.e       0.00      0.00      0.00         4\n",
      "     sn.e-CD       0.00      0.00      0.00         0\n",
      "    sn.e-SUJ       0.00      0.00      0.00       818\n",
      " sn.e.1n-SUJ       0.00      0.00      0.00         6\n",
      "       spcms       0.00      0.00      0.00       692\n",
      "       sps00       0.00      0.00      0.00      5056\n",
      "     vaic3p0       0.00      0.00      0.00         1\n",
      "     vaic3s0       0.00      0.00      0.00         4\n",
      "     vaif1p0       0.00      0.00      0.00         1\n",
      "     vaif3s0       0.00      0.00      0.00         3\n",
      "     vaii1s0       0.00      0.00      0.00         1\n",
      "     vaii3p0       0.00      0.00      0.00        13\n",
      "     vaii3s0       0.00      0.00      0.00        41\n",
      "     vaip1p0       0.00      0.00      0.00         8\n",
      "     vaip1s0       0.00      0.00      0.00         9\n",
      "     vaip2s0       0.00      0.00      0.00         0\n",
      "     vaip3p0       0.00      0.00      0.00        48\n",
      "     vaip3s0       0.00      0.00      0.00       185\n",
      "     vais3s0       0.00      0.00      0.00         5\n",
      "     vam02s0       0.00      0.00      0.00         0\n",
      "     van0000       0.00      0.00      0.00        21\n",
      "     vap00sm       0.00      0.00      0.00         1\n",
      "     vasi1p0       0.00      0.00      0.00         1\n",
      "     vasi3p0       0.00      0.00      0.00         2\n",
      "     vasi3s0       0.00      0.00      0.00        11\n",
      "     vasp1s0       0.00      0.00      0.00         1\n",
      "     vasp3p0       0.00      0.00      0.00         1\n",
      "     vasp3s0       0.00      0.00      0.00         8\n",
      "     vmg0000       0.00      0.00      0.00        92\n",
      "     vmic1p0       0.00      0.00      0.00         2\n",
      "     vmic1s0       0.00      0.00      0.00         0\n",
      "     vmic3p0       0.00      0.00      0.00        11\n",
      "     vmic3s0       0.01      0.10      0.01        30\n",
      "     vmif1p0       0.00      0.00      0.00         7\n",
      "     vmif1s0       0.00      0.00      0.00         3\n",
      "     vmif2s0       0.00      0.00      0.00         1\n",
      "     vmif3p0       0.00      0.15      0.00        40\n",
      "     vmif3s0       0.00      0.00      0.00       114\n",
      "     vmii1p0       0.00      0.00      0.00         9\n",
      "     vmii1s0       0.00      0.00      0.00         7\n",
      "     vmii2s0       0.00      0.00      0.00         0\n",
      "     vmii3p0       0.00      0.00      0.00        67\n",
      "     vmii3s0       0.00      0.00      0.00       143\n",
      "     vmip1p0       0.00      0.00      0.00        60\n",
      "     vmip1s0       0.00      0.00      0.00        60\n",
      "     vmip2s0       0.00      0.00      0.00         6\n",
      "     vmip3p0       0.00      0.00      0.00       279\n",
      "     vmip3s0       0.00      0.00      0.00       599\n",
      "     vmis1p0       0.00      0.00      0.00         5\n",
      "     vmis1s0       0.00      0.00      0.00        12\n",
      "     vmis3p0       0.00      0.00      0.00       148\n",
      "     vmis3s0       0.00      0.00      0.00       606\n",
      "     vmm02s0       0.00      0.00      0.00         3\n",
      "     vmm03p0       0.00      0.00      0.00         1\n",
      "     vmm03s0       0.00      0.00      0.00         7\n",
      "     vmn0000       0.00      0.00      0.00       849\n",
      "     vmp00pf       0.00      0.00      0.00         5\n",
      "     vmp00pm       0.00      0.00      0.00        16\n",
      "     vmp00sf       0.00      0.05      0.00        19\n",
      "     vmp00sm       0.00      0.00      0.00       311\n",
      "     vmsi1p0       0.00      0.00      0.00         2\n",
      "     vmsi1s0       0.00      0.00      0.00         2\n",
      "     vmsi3p0       0.00      0.00      0.00        12\n",
      "     vmsi3s0       0.00      0.00      0.00        26\n",
      "     vmsp1p0       0.00      0.00      0.00         9\n",
      "     vmsp1s0       0.00      0.00      0.00         7\n",
      "     vmsp2s0       0.00      0.00      0.00         0\n",
      "     vmsp3p0       0.00      0.00      0.00        45\n",
      "     vmsp3s0       0.00      0.00      0.00        66\n",
      "     vsg0000       0.00      0.00      0.00         7\n",
      "     vsic1s0       0.00      0.00      0.00         1\n",
      "     vsic3p0       0.00      0.00      0.00         2\n",
      "     vsic3s0       0.00      0.00      0.00         8\n",
      "     vsif3s0       0.00      0.00      0.00        13\n",
      "     vsii3p0       0.00      0.00      0.00        13\n",
      "     vsii3s0       0.00      0.00      0.00        26\n",
      "     vsip1p0       0.00      0.00      0.00         1\n",
      "     vsip1s0       0.00      0.00      0.00         3\n",
      "     vsip2s0       0.00      0.00      0.00         2\n",
      "     vsip3p0       0.00      0.00      0.00        48\n",
      "     vsip3s0       0.00      0.00      0.00       190\n",
      "     vsis3p0       0.00      0.00      0.00        18\n",
      "     vsis3s0       0.00      0.00      0.00        39\n",
      "     vsn0000       0.00      0.00      0.00        35\n",
      "     vsp00sm       0.00      0.00      0.00        32\n",
      "     vssi3p0       0.00      0.00      0.00         1\n",
      "     vssi3s0       0.00      0.00      0.00         3\n",
      "     vssp3p0       0.00      0.00      0.00         5\n",
      "     vssp3s0       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.00     38876\n",
      "   macro avg       0.00      0.00      0.00     38876\n",
      "weighted avg       0.00      0.00      0.00     38876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "results = pd.DataFrame(columns=['Expected', 'Predicted'])\n",
    "k = 0\n",
    "for i, lista_etiquetas_oracion in enumerate(test_tags):\n",
    "    for j, etiquetas in enumerate(lista_etiquetas_oracion):\n",
    "        k = k + 1\n",
    "        results.loc[k, 'Expected'] = etiquetas\n",
    "        results.loc[k, 'Predicted'] = log_tokens[i][j]\n",
    "\n",
    "# print(results)\n",
    "\n",
    "\n",
    "print('\\nclassification_report:\\n', classification_report(results['Expected'], results['Predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrAAFx0XrWT1"
   },
   "source": [
    "## PARTE 4  -  Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5E7-zZdGCjY"
   },
   "source": [
    "\n",
    "### Se crea una funcion que convierte el texto en una entrada para el Modelo, se genera vectores de enteros de la oracion y  ejecuta la predicion con la Entrada del modelo entrenado y el modelo de la red neuronal predice un matriz de 149 X 291 por cada oración. El shape de a predicción es (2, 149,291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "BApB6ScZ9jU8",
    "outputId": "cc225d1f-7182-4ab1-d327-b5cb265f4c63"
   },
   "outputs": [],
   "source": [
    "def postagFun(test_sample):\n",
    "    test_samples_X = []\n",
    "    for s in test_sample:\n",
    "        s_int = []\n",
    "        for w in s:\n",
    "            try:\n",
    "                s_int.append(word2index[w.lower()])\n",
    "            except KeyError:\n",
    "                s_int.append(word2index['-OOV-'])\n",
    "        test_samples_X.append(s_int)\n",
    "\n",
    "    test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "    \n",
    "    \n",
    "    predictions = model.predict(test_samples_X)\n",
    "    #print(predictions, predictions.shape)\n",
    "    \n",
    "    log_tokens = logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})\n",
    "    return log_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWp09kyGrQC"
   },
   "source": [
    "### Presentación de los Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################MODULES######################\n",
    "import sentencepiece as spm\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter.ttk import *\n",
    "\n",
    "#######################MODELS#######################\n",
    "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
    "\n",
    "spm.SentencePieceTrainer.train('--input=tweets_clean.txt --model_prefix=m_word --model_type=word --user_defined_symbols=<sep>,<cls> --vocab_size=2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "def tokenizer(post=False):\n",
    "    select = combo.get()\n",
    "    text = inputText.get(\"1.0\",END)\n",
    "    sp = spm.SentencePieceProcessor()  \n",
    "    language = ''\n",
    "\n",
    "    if(select==\"Palabras (Español)\"):\n",
    "        sp.load('m_word.model')\n",
    "        language = 'es'\n",
    "    else:\n",
    "        sp.load('m.model')\n",
    "        \n",
    "        sp.piece_to_id('<sep>')\n",
    "        sp.piece_to_id('<cls>')\n",
    "        language = 'en'\n",
    "\n",
    "        \n",
    "    res = sp.encode_as_pieces(text)\n",
    "    \n",
    "    if(post):return res,language\n",
    "    else:\n",
    "        res_postaggin=\"\"\n",
    "\n",
    "        for i in range(0,len(res)):\n",
    "            res_postaggin = res_postaggin + res[i] + \"\\n\"\n",
    "\n",
    "        outputText.configure(state='normal')\n",
    "        outputText.delete('1.0', END)\n",
    "        outputText.insert(\"insert\", res_postaggin)\n",
    "        outputText.configure(state='disabled')\n",
    "\n",
    "###################################################\n",
    "def postagging():\n",
    "        \n",
    "    res,language = tokenizer(post=True)\n",
    "    res1 = []    \n",
    "        \n",
    "    for i in range (0,len(res)):\n",
    "        if(res[i][0]=='▁' and len(res[i])>1):res1.append(res[i][1:])\n",
    "        else: res1.append(res[i])\n",
    "        \n",
    "    if(language=='es'):\n",
    "        postag = postagFun([res1])\n",
    "    else: postag = pos_tag1(res1)\n",
    "    res_postaggin=\"\"\n",
    "    \n",
    "    \n",
    "    for i in range(0,len(res)):\n",
    "        if(language=='es'):res_postaggin = res_postaggin + res1[i] +\" | \"+ postag[0][i] + \"\\n\"\n",
    "        else: res_postaggin = res_postaggin + postag[i][0] +\" | \"+ postag[i][1] + \"\\n\"\n",
    "    \n",
    "    outputText.configure(state='normal')\n",
    "    outputText.delete('1.0', END)\n",
    "    outputText.insert(\"insert\", res_postaggin)\n",
    "    outputText.configure(state='disabled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['aq0ms0', 'vmp00sf', 'vmii2s0', 'pp3fsa00', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n",
      "[('Hi', 'NP'), ('how', 'JJ'), ('are', 'NN'), ('you', 'NN'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "#######################GUI#########################\n",
    "window = Tk()\n",
    "\n",
    "window.configure(background='black')\n",
    "\n",
    "window.title(\"Procesamiento del Lenguaje Natural\")\n",
    "\n",
    "window.geometry('500x500')\n",
    "\n",
    "label= Label( text = 'Postagging-Procesamiento del Lenguaje Natural',  background = \"white\",\n",
    "            font = \"Helvetica 16 bold italic\")\n",
    "label.grid(column=0, row=0,pady=(30, 10))\n",
    "\n",
    "btn = Button(window, text=\"Tokenizer\",command=tokenizer)\n",
    "btn1 = Button(window, text=\"Postagging\",command=postagging)\n",
    "btn.grid(column=0, row=3,pady=(10, 10),padx=(0,130))\n",
    "btn1.grid(column=0, row=3,padx=(130,0))\n",
    "\n",
    "combo = Combobox(window)\n",
    "combo['values']= (\"Personalizado (Inglés) \",\"Palabras (Español)\")\n",
    "combo.current(1) #set the selected item\n",
    "combo.grid(column=0, row=1)\n",
    "\n",
    "inputText = Text(window,height=9, width=61)\n",
    "inputText.grid(column=0, row=5,padx=(2,0))\n",
    "\n",
    "outputText = Text(window,height=10, width=61, state='disabled')\n",
    "outputText.grid(column=0, row=6,padx=(2,0))\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
